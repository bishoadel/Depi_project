import os
import torch
from transformers import CLIPTokenizer, CLIPTextModel
from datasets import load_dataset
import numpy as np

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
MODEL_NAME = "runwayml/stable-diffusion-v1-5"
DATA_DIR = "./processed_coco_final"
OUTPUT_DIR = "./review_artifacts" # Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø°ÙŠ Ø³Ù†Ø¶Ø¹ ÙÙŠÙ‡ Ø£Ø¯Ù„Ø© Ø§Ù„ØªÙˆØ«ÙŠÙ‚
SAMPLES_TO_SAVE = 10 # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªÙŠ Ø³Ù†Ø­ÙØ¸Ù‡Ø§ Ù„Ù„Ø¹Ø±Ø¶

def main():
    print("ğŸš€ Starting Tokenization & Embedding Inspection...")
    
    # 1. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 2. ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Tokenizer & Text Encoder)
    print("â³ Loading CLIP Tokenizer & Text Encoder...")
    tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder="text_encoder")
    
    # 3. ØªØ­Ù…ÙŠÙ„ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print(f"ğŸ“‚ Loading dataset from {DATA_DIR}...")
    dataset = load_dataset("imagefolder", data_dir=DATA_DIR, split="train")
    
    # Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØµÙŠ (Ù„ÙŠÙ‚Ø±Ø£Ù‡ Ø§Ù„Ø¨Ø´Ø±)
    report_path = os.path.join(OUTPUT_DIR, "tokenization_report.txt")
    
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=== Tokenization & Embedding Inspection Report ===\n")
        f.write(f"Model Used: {MODEL_NAME}\n")
        f.write("=================================================\n\n")
        
        print(f"ğŸ“ Processing first {SAMPLES_TO_SAVE} samples...")
        
        for i in range(SAMPLES_TO_SAVE):
            sample = dataset[i]
            original_text = sample["text"]
            image_filename = f"sample_{i}_image.png" # Ù†Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø£ÙŠØ¶Ø§Ù‹ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
            
            # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
            sample["image"].save(os.path.join(OUTPUT_DIR, image_filename))
            
            # --- Ø§Ù„Ø®Ø·ÙˆØ© 1: Tokenization ---
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… (Input IDs)
            inputs = tokenizer(
                original_text, 
                padding="max_length", 
                max_length=tokenizer.model_max_length, 
                truncation=True, 
                return_tensors="pt"
            )
            input_ids = inputs.input_ids
            
            # Ø¥Ø¹Ø§Ø¯Ø© ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± (Ù„Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„ØªÙˆÙƒÙ†Ø² ØµØ­ÙŠØ­Ø©)
            decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
            
            # --- Ø§Ù„Ø®Ø·ÙˆØ© 2: Embeddings ---
            # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
            with torch.no_grad():
                outputs = text_encoder(input_ids)
                last_hidden_state = outputs.last_hidden_state # Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ù€ Embeddings
            
            # --- ÙƒØªØ§Ø¨Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ± ---
            f.write(f"Sample #{i+1}:\n")
            f.write(f"Original Text:  {original_text}\n")
            f.write(f"Token IDs:      {input_ids[0].numpy().tolist()[:15]} ... (truncated)\n") # Ù†Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 15 Ø±Ù‚Ù… ÙÙ‚Ø·
            f.write(f"Decoded Check:  {decoded_text}\n")
            f.write(f"Embedding Shape: {last_hidden_state.shape}  <-- (Batch, Sequence Length, Vector Dim)\n")
            f.write("-" * 50 + "\n")
            
            # --- Ø­ÙØ¸ Ø§Ù„Ù€ Embeddings ÙƒÙ…Ù„Ù ØªÙ‚Ù†ÙŠ (.pt) ---
            # Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ØªØ«Ø¨Øª Ø¨Ù‡ Ø£Ù†Ùƒ Ø§Ø³ØªØ®Ø±Ø¬Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª ÙØ¹Ù„ÙŠØ§Ù‹
            embedding_path = os.path.join(OUTPUT_DIR, f"sample_{i}_embedding.pt")
            torch.save(last_hidden_state, embedding_path)

    print("\nâœ… DONE! Artifacts saved.")
    print(f"ğŸ“„ Report File: {os.path.abspath(report_path)}")
    print(f"ğŸ“‚ Embeddings Tensors: {os.path.abspath(OUTPUT_DIR)}")
    print("ğŸ’¡ You can open 'tokenization_report.txt' now to review the steps.")

if __name__ == "__main__":
    main()