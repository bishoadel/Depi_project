import os
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from datasets import load_dataset
from diffusers import DDPMScheduler, UNet2DConditionModel, DiffusionPipeline, AutoencoderKL
from transformers import CLIPTextModel, CLIPTokenizer
from torchvision import transforms
from tqdm.auto import tqdm
import logging
import math

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ---
MODEL_NAME = "runwayml/stable-diffusion-v1-5"
OUTPUT_DIR = "./sd-model-finetuned"
DATA_DIR = "./processed_coco_final"

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù€ GPU
TRAIN_BATCH_SIZE = 4 
GRADIENT_ACCUMULATION_STEPS = 1
LEARNING_RATE = 1e-5
NUM_EPOCHS = 1 

# =========================================================
# ğŸ”¥ Ø¥ØµÙ„Ø§Ø­ ÙˆÙŠÙ†Ø¯ÙˆØ²: ØªØ¹Ø±ÙŠÙ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§Ø±Ø¬ Ø§Ù„Ù€ Main
# =========================================================

# 1. ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù€ Tokenizer Ø¹Ø§Ù„Ù…ÙŠØ§Ù‹
tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder="tokenizer")

# 2. ØªØ¹Ø±ÙŠÙ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØ± Ø¹Ø§Ù„Ù…ÙŠØ§Ù‹
train_transforms = transforms.Compose([
    transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.CenterCrop(512),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5]),
])

# 3. Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ØµØ¨Ø­Øª Ø¹Ø§Ù„Ù…ÙŠØ© (Global Function)
def preprocess_train(examples):
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±
    images = [train_transforms(image.convert("RGB")) for image in examples["image"]]
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
    text_inputs = tokenizer(
        examples["text"], 
        padding="max_length", 
        max_length=tokenizer.model_max_length, 
        truncation=True, 
        return_tensors="pt"
    )
    return {
        "pixel_values": images,
        "input_ids": text_inputs.input_ids,
    }
# =========================================================


def main():
    # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø±Ø¹
    accelerator = Accelerator(
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        mixed_precision="fp16" 
    )

    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )

    if accelerator.is_main_process:
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        print("\n" + "="*40)
        print(f"ğŸš€ STARTING PROJECT: Text-to-Image Generation")
        print(f"ğŸ–¥ï¸  GPU: RTX 3090 Detected (Windows Fix Applied)")
        print("="*40 + "\n")

    # 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø© (Ø¯Ø§Ø®Ù„ Main Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©)
    noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder="scheduler")
    text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder="unet")

    # Ø§Ù„ØªØ¬Ù…ÙŠØ¯ (Freeze)
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    unet.train()

    # 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    if accelerator.is_main_process:
        print(f"â³ Loading Dataset from {DATA_DIR}...")

    dataset = load_dataset("imagefolder", data_dir=DATA_DIR, split="train")

    # ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø§Ù„ØªÙŠ Ø£ØµØ¨Ø­Øª global Ø§Ù„Ø¢Ù†)
    with accelerator.main_process_first():
        train_dataset = dataset.with_transform(preprocess_train)

    # DataLoader
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2
    )

    # 4. Ø§Ù„Ù…Ø¬Ù‡Ø² (Optimizer)
    optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)

    # Ø§Ù„ØªØ­Ø¶ÙŠØ±
    unet, optimizer, train_dataloader = accelerator.prepare(
        unet, optimizer, train_dataloader
    )
    
    text_encoder.to(accelerator.device, dtype=torch.float16)
    vae.to(accelerator.device, dtype=torch.float16)

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·ÙˆØ§Øª
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / GRADIENT_ACCUMULATION_STEPS)
    max_train_steps = NUM_EPOCHS * num_update_steps_per_epoch

    # 5. Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    if accelerator.is_main_process:
        print(f"\nâœ… Training Started (Total Steps: {max_train_steps})... GO! ğŸ‘‡")

    global_step = 0
    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process, desc="Training Progress", unit="step")

    for epoch in range(NUM_EPOCHS):
        unet.train()
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(unet):
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ± Latents
                latents = vae.encode(batch["pixel_values"].to(dtype=torch.float16)).latent_dist.sample()
                latents = latents * vae.config.scaling_factor

                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
                noise = torch.randn_like(latents)
                bsz = latents.shape[0]
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()

                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                
                # Ø§Ù„Ù†Øµ
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]

                # Ø§Ù„ØªÙ†Ø¨Ø¤
                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
                
                # Ø§Ù„Ø®Ø³Ø§Ø±Ø©
                loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")

                # Ø§Ù„ØªØ­Ø¯ÙŠØ«
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()

            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                progress_bar.set_postfix({"loss": f"{loss.detach().item():.4f}"})

    # 6. Ø§Ù„Ø­ÙØ¸
    if accelerator.is_main_process:
        print("\nâ³ Saving Final Model...")
        pipeline = DiffusionPipeline.from_pretrained(
            MODEL_NAME,
            unet=accelerator.unwrap_model(unet),
            text_encoder=text_encoder,
            vae=vae,
            tokenizer=tokenizer,
        )
        pipeline.save_pretrained(OUTPUT_DIR)
        print("\n" + "="*40)
        print(f"ğŸ‰ MISSION ACCOMPLISHED!")
        print(f"ğŸ“‚ Model saved at: {os.path.abspath(OUTPUT_DIR)}")
        print("="*40)

if __name__ == "__main__":
    main()